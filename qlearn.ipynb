{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import gym_tic_tac_toe\n",
    "import plotting\n",
    "from plotting import EpisodeStats\n",
    "from collections import defaultdict \n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Q dictionary used in q_learn function\n",
    "#\n",
    "# Q = {\n",
    "#     \"0000000001\": {\n",
    "#         0: 0,\n",
    "#         1: 0,\n",
    "#         2: 0,\n",
    "#         # ...\n",
    "#         8: 0\n",
    "#     },\n",
    "#     # ...\n",
    "#     \"1-11-1-11-11-10-1\": {\n",
    "#         8: 0\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_state(state):\n",
    "    board = state['board']\n",
    "    move = state['on_move']\n",
    "    return ''.join(str(b) for b in board)+str(move)\n",
    "    \n",
    "def hash_action(action):\n",
    "    return action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action_idx(Q, state_hash, action_hashes):\n",
    "    if state_hash in Q:\n",
    "        best_action_idx = np.argmax(Q[state_hash]) \n",
    "    else:\n",
    "        Q[state_hash] = dict((ah, 0) for ah in action_hashes)\n",
    "        best_action_idx = np.random.choice(len(action_hashes))\n",
    "        \n",
    "    return best_action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(Q, epsilon):\n",
    "\n",
    "    def get_action_probs(state_hash, action_hashes):\n",
    "        num_actions = len(action_hashes)\n",
    "        action_probs = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "\n",
    "        best_action_idx = get_best_action_idx(Q, state_hash, action_hashes)\n",
    "        action_probs[best_action_idx] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "   \n",
    "    return get_action_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(init_state, num_episodes, discount_factor = 1.0, alpha = 0.6, epsilon = 0.1, print_log = False): \n",
    "    env = gym.make('tic_tac_toe-v1')\n",
    " \n",
    "    Q = {}\n",
    "    \n",
    "    stats = plotting.EpisodeStats( \n",
    "        episode_lengths = np.zeros(num_episodes), \n",
    "        episode_rewards = np.zeros(num_episodes))\n",
    "    \n",
    "    policy = create_policy(Q, epsilon) \n",
    "       \n",
    "    for ith_episode in range(num_episodes): \n",
    "           \n",
    "        env.set_state(deepcopy(init_state))\n",
    "        state = deepcopy(env.state)\n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            state_hash = hash_state(state)\n",
    "            actions = env.move_generator()\n",
    "            action_hashes = [hash_action(act) for act in actions]\n",
    "            action_probabilities = policy(state_hash, action_hashes)\n",
    "            \n",
    "            action_idx = np.random.choice(np.arange(len(actions)), p=action_probabilities)\n",
    "            action = actions[action_idx]\n",
    "            action_hash = hash_action(action)\n",
    "                           \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "   \n",
    "            stats.episode_rewards[ith_episode] += reward \n",
    "            stats.episode_lengths[ith_episode] = t \n",
    "            \n",
    "            next_state_hash = hash_state(next_state)\n",
    "            next_action_hashes = [hash_action(act) for act in env.move_generator()]\n",
    "            \n",
    "            if len(next_action_hashes) == 0:\n",
    "                next_action_score = 0\n",
    "            else:   \n",
    "                best_next_action_idx = get_best_action_idx(Q, next_state_hash, next_action_hashes)\n",
    "                best_next_action_hash = next_action_hashes[best_next_action_idx]\n",
    "                next_action_score = Q[next_state_hash][best_next_action_hash]\n",
    "                \n",
    "            td_target = reward + discount_factor * next_action_score\n",
    "            td_delta = td_target - Q[state_hash][action_hash] \n",
    "            Q[state_hash][action_hash] += alpha * td_delta\n",
    "            \n",
    "            if print_log:\n",
    "                print('\\n\\n----------- STATE -------------')\n",
    "                env.render()\n",
    "                print(state_hash)\n",
    "                print(actions)\n",
    "                print(action_hashes)\n",
    "                print(action_probabilities)\n",
    "                print(action_idx)\n",
    "                print(action)\n",
    "                print(action_hash)\n",
    "                print(reward)\n",
    "                print(td_target)\n",
    "                print(td_delta)\n",
    "                print(done)\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "                   \n",
    "            state = deepcopy(next_state)\n",
    "       \n",
    "    return Q, stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(player = -1):\n",
    "    env = gym.make('tic_tac_toe-v1')\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    on_move = state['on_move']\n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        on_move = state['on_move']\n",
    "        \n",
    "        if player == on_move:\n",
    "            print('Pick a move index')\n",
    "            moves = env.move_generator()\n",
    "            print(list(enumerate(moves)))\n",
    "            idx = int(input())\n",
    "            action = moves[idx]\n",
    "        else:\n",
    "            (Q, stats) = q_learn(state, 1000, print_log=False)\n",
    "            actions = Q[hash_state(state)].items()\n",
    "            print(actions)\n",
    "            best_action_hash = max(actions, key=operator.itemgetter(1))\n",
    "            print(best_action_hash)\n",
    "            best_action_hash = best_action_hash[0]\n",
    "            action = [on_move, best_action_hash]\n",
    "        \n",
    "        state, reward, done, _ = env.step(action) \n",
    "\n",
    "        env.render()\n",
    "    \n",
    "    if reward == 0:\n",
    "        print(\"Draw!\")\n",
    "    elif on_move == player:\n",
    "        print('You won!')\n",
    "    else:\n",
    "        print('AI won!')\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on move:  X\n",
      "      \n",
      "      \n",
      "      \n",
      "dict_items([(0, 1.0), (1, 0.8112168848224419), (2, 0.9345409537075053), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, -0.31176360345599996), (8, -0.59558592135168)])\n",
      "(0, 1.0)\n",
      "on move:  O\n",
      "X     \n",
      "      \n",
      "      \n",
      "Pick a move index\n",
      "[(0, [-1, 1]), (1, [-1, 2]), (2, [-1, 3]), (3, [-1, 4]), (4, [-1, 5]), (5, [-1, 6]), (6, [-1, 7]), (7, [-1, 8])]\n",
      "on move:  X\n",
      "X     \n",
      "  O   \n",
      "      \n",
      "dict_items([(1, 1.0), (2, 0.991816266075178), (3, 0.9998111942734078), (5, -0.3884312508825599), (6, -0.9947428103827276), (7, 0.9315871545674642), (8, 0.930459488256)])\n",
      "(1, 1.0)\n",
      "on move:  O\n",
      "X X   \n",
      "  O   \n",
      "      \n",
      "Pick a move index\n",
      "[(0, [-1, 2]), (1, [-1, 3]), (2, [-1, 5]), (3, [-1, 6]), (4, [-1, 7]), (5, [-1, 8])]\n",
      "on move:  X\n",
      "X X   \n",
      "  O   \n",
      "O     \n",
      "dict_items([(8, -0.999605735424), (2, 1.0), (3, -0.9999962204287794), (5, -0.9999991891101745), (7, -0.9999786325377025)])\n",
      "(2, 1.0)\n",
      "on move:  O\n",
      "X X X \n",
      "  O   \n",
      "O     \n",
      "AI won!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gym_tic_tac_toe.envs.tic_tac_toe_env.TicTacToeEnv at 0x7f4304a71a90>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
