{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import gym_tic_tac_toe\n",
    "import plotting\n",
    "from plotting import EpisodeStats\n",
    "from collections import defaultdict \n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Q dictionary used in q_learn function\n",
    "#\n",
    "# Q = {\n",
    "#     \"0000000001\": {\n",
    "#         0: 0,\n",
    "#         1: 0,\n",
    "#         2: 0,\n",
    "#         # ...\n",
    "#         8: 0\n",
    "#     },\n",
    "#     # ...\n",
    "#     \"1-11-1-11-11-10-1\": {\n",
    "#         8: 0\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# ile zajmuje wyuczenie sie\n",
    "# procent klasyfikacji, dokładność, \n",
    "# jak zależy od rozmiaru zbioru uczacego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_state(state):\n",
    "    board = state['board']\n",
    "    move = state['on_move']\n",
    "    return ''.join(str(b) for b in board)+str(move)\n",
    "    \n",
    "def hash_action(action):\n",
    "    return action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action_idx(Q, state_hash, action_hashes):\n",
    "    if state_hash in Q:\n",
    "        best_action_idx = np.argmax(Q[state_hash]) \n",
    "    else:\n",
    "        Q[state_hash] = dict((ah, 0) for ah in action_hashes)\n",
    "        best_action_idx = np.random.choice(len(action_hashes))\n",
    "        \n",
    "    return best_action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(Q, epsilon):\n",
    "\n",
    "    def get_action_probs(state_hash, action_hashes):\n",
    "        num_actions = len(action_hashes)\n",
    "        action_probs = np.ones(num_actions, dtype = float) * epsilon / num_actions \n",
    "        best_action_idx = get_best_action_idx(Q, state_hash, action_hashes)\n",
    "        action_probs[best_action_idx] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "   \n",
    "    return get_action_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(num_episodes, discount_factor = 1.0, alpha = 0.6, epsilon = 0.1, print_log = False):  \n",
    "    Q = {}\n",
    "    env = gym.make('tic_tac_toe-v1')\n",
    "    \n",
    "    stats = plotting.EpisodeStats( \n",
    "        episode_lengths = np.zeros(num_episodes), \n",
    "        episode_rewards = np.zeros(num_episodes))\n",
    "    \n",
    "    policy = create_policy(Q, epsilon) \n",
    "       \n",
    "    for ith_episode in range(num_episodes): \n",
    "        if (ith_episode%1000==0):\n",
    "            print(ith_episode)\n",
    "        env.reset()\n",
    "        state = deepcopy(env.state)\n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            state_hash = hash_state(state)\n",
    "            actions = env.move_generator()\n",
    "            action_hashes = [hash_action(act) for act in actions]\n",
    "            action_probabilities = policy(state_hash, action_hashes)\n",
    "            \n",
    "            action_idx = np.random.choice(np.arange(len(actions)), p=action_probabilities)\n",
    "            action = actions[action_idx]\n",
    "            action_hash = hash_action(action)\n",
    "                           \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "   \n",
    "            stats.episode_rewards[ith_episode] += reward \n",
    "            stats.episode_lengths[ith_episode] = t \n",
    "            \n",
    "            next_state_hash = hash_state(next_state)\n",
    "            next_action_hashes = [hash_action(act) for act in env.move_generator()]\n",
    "            \n",
    "            if len(next_action_hashes) == 0:\n",
    "                next_action_score = 0\n",
    "            else:   \n",
    "                best_next_action_idx = get_best_action_idx(Q, next_state_hash, next_action_hashes)\n",
    "                best_next_action_hash = next_action_hashes[best_next_action_idx]\n",
    "                next_max = Q[next_state_hash][best_next_action_hash]\n",
    "                \n",
    "            old_value = Q[state_hash][action_hash]\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + discount_factor * next_max)\n",
    "            Q[state_hash][action_hash] = new_value\n",
    "            \n",
    "            if print_log:\n",
    "                print('\\n\\n----------- STATE -------------')\n",
    "                env.render()\n",
    "                print(state_hash)\n",
    "                print(actions)\n",
    "                print(action_hashes)\n",
    "                print(action_probabilities)\n",
    "                print(action_idx)\n",
    "                print(action)\n",
    "                print(action_hash)\n",
    "                print(reward)\n",
    "                print(td_target)\n",
    "                print(td_delta)\n",
    "                print(done)\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "                   \n",
    "            state = deepcopy(next_state)\n",
    "       \n",
    "    return Q, stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def play_game(Q, player = -1):\n",
    "    env = gym.make('tic_tac_toe-v1')\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    on_move = state['on_move']\n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        on_move = state['on_move']\n",
    "        \n",
    "        if player == on_move:\n",
    "            print('Pick a move index')\n",
    "            moves = env.move_generator()\n",
    "            print(list(enumerate(moves)))\n",
    "            idx = int(input())\n",
    "            action = moves[idx]\n",
    "        else:\n",
    "            actions = Q[hash_state(state)].items()\n",
    "            print(actions)\n",
    "            best_action_hash = max(actions, key=operator.itemgetter(1))\n",
    "            print(best_action_hash)\n",
    "            best_action_hash = best_action_hash[0]\n",
    "            action = [on_move, best_action_hash]\n",
    "        \n",
    "        state, reward, done, _ = env.step(action) \n",
    "\n",
    "        env.render()\n",
    "    \n",
    "    if reward == 0:\n",
    "        print(\"Draw!\")\n",
    "    elif on_move == player:\n",
    "        print('You won!')\n",
    "    else:\n",
    "        print('AI won!')\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n"
     ]
    }
   ],
   "source": [
    "(Q, stats) = q_learn(30000, print_log=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on move:  X\n",
      "      \n",
      "      \n",
      "      \n",
      "dict_items([(0, 1.0), (1, 1.0), (2, 1.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, -1.0), (8, -1.0)])\n",
      "(0, 1.0)\n",
      "on move:  O\n",
      "X     \n",
      "      \n",
      "      \n",
      "Pick a move index\n",
      "[(0, [-1, 1]), (1, [-1, 2]), (2, [-1, 3]), (3, [-1, 4]), (4, [-1, 5]), (5, [-1, 6]), (6, [-1, 7]), (7, [-1, 8])]\n",
      "on move:  X\n",
      "X     \n",
      "  O   \n",
      "      \n",
      "dict_items([(1, 1.0), (2, 0.9984246610803995), (3, 0.9188485652062178), (5, -0.7784503039561823), (6, 0.0), (7, 43.621953019776925), (8, 0.9061484927998593)])\n",
      "(7, 43.621953019776925)\n",
      "on move:  O\n",
      "X     \n",
      "  O   \n",
      "  X   \n",
      "Pick a move index\n",
      "[(0, [-1, 1]), (1, [-1, 2]), (2, [-1, 3]), (3, [-1, 5]), (4, [-1, 6]), (5, [-1, 8])]\n",
      "on move:  X\n",
      "X   O \n",
      "  O   \n",
      "  X   \n",
      "dict_items([(8, 0), (1, 0.0), (3, 0), (5, 0), (6, 0)])\n",
      "(8, 0)\n",
      "on move:  O\n",
      "X   O \n",
      "  O   \n",
      "  X X \n",
      "Pick a move index\n",
      "[(0, [-1, 1]), (1, [-1, 3]), (2, [-1, 5]), (3, [-1, 6])]\n"
     ]
    }
   ],
   "source": [
    "play_game(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
